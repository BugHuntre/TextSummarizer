TrainingArguments:
  num_train_epochs: 1
  warmup_steps: 200        # lower warmup to save training time
  per_device_train_batch_size: 1
  weight_decay: 0.01
  logging_steps: 50        # less frequent logs (lighter on I/O)
  eval_strategy: steps
  eval_steps: 1000         # evaluate less often to reduce GPU load
  save_steps: 1000000
  gradient_accumulation_steps: 8   # reduce accumulation to save VRAM
